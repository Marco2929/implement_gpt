# Self implementation of a transformer

My personal repository to learn LLMs/GPTs from scratch and constantly improving It. The projects base is the video series of Andrej Karpathy

## Todo:

- [x] Basic implementation according to AK video
- [x] Implement it by my self using just comments and attention is all you need
- Add different tokenizers and compare them 
  - [ ] BPE
  - [ ] T-Free (uses a different tokenizer technique)
- [ ] Use different dataset
- [ ] Add different pre-trained embeddings models and compare them
  - [ ] AnglE
  - [ ] gte-large-en-v1.5
- [ ] Improve training process and comparsion
- [ ] Add rotary embeddings
- [ ] Grouped query attention
- [ ] Flash attention
- [ ] Activation function